{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "# %conda install matplotlib-inline\n",
    "# %conda install pandas\n",
    "# %conda install numpy\n",
    "# %conda install scikit-learn\n",
    "# %conda install scipy\n",
    "# %conda install matplotlib\n",
    "# %conda install seaborn\n",
    "# %conda install tqdm\n",
    "# %pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np \n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1731317034561,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -60
    },
    "id": "j-SP8NPsMNRL",
    "outputId": "5e6611fb-7efd-47e9-9b23-715842479679"
   },
   "outputs": [],
   "source": [
    "# create synthetic data\n",
    "\n",
    "N = 30\n",
    "x = torch.randn(N, 1)\n",
    "y = x + torch.randn(N, 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    \"s\",\n",
    "    color=\"#C6A0F6\",\n",
    "    markersize=8,\n",
    "    markeredgecolor=\"#a6adc8\",\n",
    "    markeredgewidth=1,\n",
    ")\n",
    "ax.set_xlabel(\"x\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"y\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_title(\"Training Data\", fontsize=16, color=\"#ed8796\", fontweight=\"bold\", pad=15)\n",
    "ax.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax.set_facecolor(\"#1e1e2e\")\n",
    "ax.tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Style spines\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1731317034561,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -60
    },
    "id": "krQeh5wYMNla",
    "outputId": "bdbc2aa0-2c47-41d8-c65a-8856f6ab9577"
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "ANNreg = nn.Sequential(\n",
    "    nn.Linear(1, 1),  # hidden layer (1 input, 1 output)\n",
    "    nn.ReLU(),  # activation function\n",
    "    nn.Linear(1, 1),  # output layer (1 input, 1 output)\n",
    ")\n",
    "# Notice that we express linear operations in a separate call than the activation function despite being in the same layer.\n",
    "ANNreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common methods associated with nn.Module\n",
    "\n",
    "# .train(): Puts the model in training mode\n",
    "# .eval(): Puts the model in evaluation mode\n",
    "# .requires_grad_(True/False): Turns on/off gradient tracking\n",
    "# .zero_grad(): Clears the gradients of all parameters\n",
    "# .backward(): Computes the gradient of the loss with respect to the parameters\n",
    "# .step(): Updates the parameters of the model\n",
    "# .state_dict(): Returns a dictionary containing the model's parameters\n",
    "\n",
    "# .parameters(): Returns an iterator over the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNreg.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmHh7GrvMNoy"
   },
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learningRate = 0.05\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.MSELoss()  # Mean Squared Error Loss\n",
    "\n",
    "# optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(\n",
    "    ANNreg.parameters(),  # Which parameters to optimize\n",
    "    lr=learningRate,  # Learning rate\n",
    ")\n",
    "\n",
    "# What's the difference between gradient descent and stochastic gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Gradient descent**, in its purest form, also known as **batch gradient descent**, calculates the error for all the training examples before making a single update to the model's parameters. This means it computes the average gradient across the entire dataset to take one step towards the minimum of the cost function.\n",
    "\n",
    "**Stochastic gradient descent**, on the other hand, takes a more granular approach. It updates the model's parameters for each training example it processes. Instead of averaging the gradients of all examples, it takes a \"stochastic\" or random sample of one example at a time to guide its descent.\n",
    "\n",
    "\n",
    "### The Impact of Batch Size\n",
    "\n",
    "The distinction between these two methods can be further understood by the concept of **batch size**, which is the number of training examples utilized in one iteration.\n",
    "\n",
    "* **Gradient Descent:** The batch size is equal to the total number of training examples.\n",
    "* **Stochastic Gradient Descent:** The batch size is one.\n",
    "\n",
    "### The \"Noisy\" Advantage of SGD\n",
    "\n",
    "A key characteristic of SGD is its \"noisy\" updates. Because it's learning from one example at a time, the path it takes towards the minimum can be erratic and jumpy. While this might seem like a disadvantage, it can actually be beneficial. This randomness can help the algorithm escape from shallow **local minima** in the cost function, which are points that appear to be a minimum but aren't the true lowest point (the **global minimum**). Batch gradient descent, with its smooth and direct path, is more susceptible to getting stuck in these local minima.\n",
    "\n",
    "\n",
    "\n",
    "### Which One Should You Choose?\n",
    "\n",
    "* **Mini-Batch Gradient Descent:** This is a popular hybrid approach where the batch size is greater than one but less than the total number of training examples. It strikes a balance between the computational efficiency of SGD and the stable convergence of batch gradient descent.\n",
    "In practice, **mini-batch gradient descent** is the most commonly used variant as it combines the best of both worlds, offering a good balance between computational speed and convergence stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmraVzTcJ0x1"
   },
   "outputs": [],
   "source": [
    "# Loss Landscape Visualization: Local vs Global Minima\n",
    "# This shows a mathematical function with multiple minima\n",
    "\n",
    "\n",
    "def create_loss_landscape():\n",
    "    \"\"\"\n",
    "    Creates a visualization of a loss function with local and global minima\n",
    "    to illustrate the concept of optimization landscapes\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up Catppuccin Macchiato color scheme\n",
    "    colors = {\n",
    "        \"text\": \"#cad3f5\",\n",
    "        \"title\": \"#ed8796\",\n",
    "        \"function\": \"#a6da95\",\n",
    "        \"global_min\": \"#C6A0F6\",\n",
    "        \"local_min1\": \"#f5a97f\",\n",
    "        \"local_min2\": \"#8bd5ca\",\n",
    "        \"grid\": \"#cad3f5\",\n",
    "    }\n",
    "\n",
    "    # Create parameter range\n",
    "    theta = np.linspace(-3, 5, 1000)\n",
    "\n",
    "    # Define a loss function with multiple minima\n",
    "    # Global minimum around -5, with several local minima at higher values\n",
    "    loss = (\n",
    "        # Main global minimum (deep valley at theta=3.5)\n",
    "        2 * np.exp(-3 * (theta - 3.5) ** 2) * (-8)\n",
    "        +\n",
    "        # Local minimum (at theta=-1)\n",
    "        1.5 * np.exp(-2 * (theta + 1) ** 2) * (-2)\n",
    "        +\n",
    "        # Another local minimum (at theta=1)\n",
    "        1 * np.exp(-4 * (theta - 1) ** 2) * (-1.5)\n",
    "        +\n",
    "        # Base function to create hills and valleys\n",
    "        0.3 * theta**2\n",
    "        + 1.5 * np.sin(2 * theta)\n",
    "        + 8\n",
    "    )\n",
    "\n",
    "    # Find critical points\n",
    "    global_min_idx = np.argmin(loss)\n",
    "    global_min_theta = theta[global_min_idx]\n",
    "    global_min_loss = loss[global_min_idx]\n",
    "\n",
    "    # Create the plot with dark background\n",
    "    plt.style.use(\"dark_background\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), facecolor=\"#1e1e2e\")\n",
    "    fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "    # Plot the loss function\n",
    "    ax.plot(\n",
    "        theta, loss, color=colors[\"function\"], linewidth=3, label=\"Loss Function L(θ)\"\n",
    "    )\n",
    "\n",
    "    # Mark the global minimum\n",
    "    ax.plot(\n",
    "        global_min_theta,\n",
    "        global_min_loss,\n",
    "        \"o\",\n",
    "        color=colors[\"global_min\"],\n",
    "        markersize=15,\n",
    "        label=f\"Global Minimum\\nθ* = {global_min_theta:.2f}\\nL(θ*) = {global_min_loss:.1f}\",\n",
    "    )\n",
    "\n",
    "    # Mark local minima\n",
    "    local_minima_positions = [-1.0, 1.0]\n",
    "    local_minima_colors = [colors[\"local_min1\"], colors[\"local_min2\"]]\n",
    "\n",
    "    for i, lm_theta in enumerate(local_minima_positions):\n",
    "        lm_idx = np.argmin(np.abs(theta - lm_theta))\n",
    "        lm_loss = loss[lm_idx]\n",
    "        ax.plot(\n",
    "            lm_theta,\n",
    "            lm_loss,\n",
    "            \"o\",\n",
    "            color=local_minima_colors[i],\n",
    "            markersize=12,\n",
    "            label=f\"Local Minimum {i+1}\\nθ = {lm_theta:.1f}\\nL(θ) = {lm_loss:.1f}\",\n",
    "        )\n",
    "\n",
    "    # Styling to match your theme\n",
    "    ax.set_xlabel(\"Parameter θ\", fontsize=16, color=colors[\"text\"], fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Loss L(θ)\", fontsize=16, color=colors[\"text\"], fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Loss Landscape: Local vs Global Minima\",\n",
    "        fontsize=18,\n",
    "        color=colors[\"title\"],\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3, color=colors[\"grid\"], linestyle=\"--\")\n",
    "    ax.legend(fontsize=12, framealpha=0.9, facecolor=\"#181825\")\n",
    "    ax.set_facecolor(\"#1e1e2e\")\n",
    "    ax.tick_params(colors=colors[\"text\"], labelsize=14)\n",
    "\n",
    "    # Remove the legend\n",
    "    ax.legend().remove()\n",
    "\n",
    "    # Style spines\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(colors[\"text\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the visualization\n",
    "create_loss_landscape()\n",
    "\n",
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of9E8ClxMNsD"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "numepochs = 500\n",
    "losses = torch.zeros(numepochs)  # Initialize losses vector\n",
    "\n",
    "\n",
    "## Train the model!\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    yHat = ANNreg(x)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(yHat, y)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()  # Calculate new gradients via backprop\n",
    "    optimizer.step()  # Update the parameters using gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1731317035246,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -60
    },
    "id": "zmX6K49WMNuy",
    "outputId": "245b0f3d-93d3-4991-b456-f434a5e4fb9d"
   },
   "outputs": [],
   "source": [
    "# final forward pass throught the trained model\n",
    "predictions = ANNreg(x)\n",
    "\n",
    "# Test loss of the trained model (MSE)\n",
    "testloss = (predictions - y).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses over epochs\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "ax.plot(\n",
    "    losses.detach(),\n",
    "    \"o\",\n",
    "    color=\"#a6da95\",\n",
    "    markerfacecolor=\"#a6da95\",\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    linewidth=0.5,\n",
    "    markersize=4,\n",
    ")\n",
    "ax.plot(\n",
    "    numepochs,\n",
    "    testloss.detach(),\n",
    "    \"o\",\n",
    "    color=\"#f5a97f\",\n",
    "    markersize=10,\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=1.5,\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Epoch\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Loss\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Training Loss (Final loss = {testloss.item():.4f})\",\n",
    "    fontsize=16,\n",
    "    color=\"#ed8796\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax.set_facecolor(\"#1e1e2e\")\n",
    "ax.tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Style spines\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1731317035521,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -60
    },
    "id": "i1TCt0mpMNxC",
    "outputId": "0a961c92-5b7f-4e0d-e6aa-510cd645d79f"
   },
   "outputs": [],
   "source": [
    "# Plot predictions vs real data with Catppuccin Macchiato styling\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(figsize=(10, 7), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(y.T, predictions.detach().T)[0, 1]\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    y,\n",
    "    \"o\",\n",
    "    color=\"#C6A0F6\",\n",
    "    markersize=8,\n",
    "    markeredgecolor=\"#a6adc8\",\n",
    "    markeredgewidth=1,\n",
    "    label=\"Real data\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    predictions.detach(),\n",
    "    \"s\",\n",
    "    color=\"#f5a97f\",\n",
    "    markersize=8,\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=1,\n",
    "    label=\"Predictions\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"y\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Model Predictions vs Real Data (r = {correlation:.3f})\",\n",
    "    fontsize=16,\n",
    "    color=\"#ed8796\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax.legend(fontsize=12, framealpha=0.9, facecolor=\"#181825\", edgecolor=\"#cad3f5\")\n",
    "ax.set_facecolor(\"#1e1e2e\")\n",
    "ax.tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Style spines\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We randomly selected N of 30 data points but can we more systematically answer how much data is enough ?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create two functions, one to create the data and one to build and train the model.\n",
    "\n",
    "\n",
    "def createTheData(N):\n",
    "    x = torch.randn(N, 1)\n",
    "    y = x + torch.randn(N, 1) / 5\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def buildAndTrainTheModel(x, y):\n",
    "\n",
    "    # build the model\n",
    "    ANNreg = nn.Sequential(\n",
    "        nn.Linear(1, 1),  # input layer\n",
    "        nn.LeakyReLU(),  # activation function\n",
    "        nn.Linear(1, 1),  # output layer\n",
    "    )\n",
    "\n",
    "    # loss and optimizer functions\n",
    "    lossfun = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ANNreg.parameters(), lr=0.05)\n",
    "\n",
    "    #### train the model\n",
    "    numepochs = 500\n",
    "    losses = torch.zeros(numepochs)\n",
    "\n",
    "    for epochi in range(numepochs):\n",
    "\n",
    "        # forward pass\n",
    "        yHat = ANNreg(x)\n",
    "\n",
    "        # compute loss\n",
    "        loss = lossfun(yHat, y)\n",
    "        losses[epochi] = loss\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # end training loop\n",
    "\n",
    "    ### compute model predictions\n",
    "    predictions = ANNreg(x)\n",
    "\n",
    "    # output:\n",
    "    return predictions, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the function over a range of N values and save the losses\n",
    "\n",
    "N_values = []\n",
    "test_losses = []\n",
    "\n",
    "for n in range(5, 500, 5):\n",
    "    x, y = createTheData(n)\n",
    "    predictions, losses = buildAndTrainTheModel(x, y)\n",
    "    testloss = (predictions - y).pow(2).mean()\n",
    "    N_values.append(n)\n",
    "    test_losses.append(testloss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses over N\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "ax.plot(\n",
    "    N_values,\n",
    "    test_losses,\n",
    "    \"o-\",\n",
    "    color=\"#a6da95\",\n",
    "    linewidth=3,\n",
    "    markersize=8,\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=0.5,\n",
    ")\n",
    "ax.set_xlabel(\n",
    "    \"Number of data points (N)\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\"\n",
    ")\n",
    "ax.set_ylabel(\"Test loss\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Test Loss vs Number of Data Points\",\n",
    "    fontsize=16,\n",
    "    color=\"#ed8796\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax.set_facecolor(\"#1e1e2e\")\n",
    "ax.tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Style spines\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's try a a different y equation, there's a point to make here I promise.\n",
    "\n",
    "\n",
    "def createTheData(m):\n",
    "    N = 20\n",
    "    x = torch.randn(N, 1)\n",
    "    y = m * x + torch.randn(N, 1) / 2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the model over a range of m values\n",
    "\n",
    "\n",
    "# the slopes to simulate\n",
    "slopes = np.linspace(-2, 2, 21)\n",
    "\n",
    "numExps = 50\n",
    "\n",
    "# initialize output matrix\n",
    "results = np.zeros((len(slopes), numExps, 2))\n",
    "\n",
    "for slope_idx in range(len(slopes)):\n",
    "\n",
    "    for N in range(numExps):\n",
    "\n",
    "        # create a dataset and run the model\n",
    "        x, y = createTheData(slopes[slope_idx])\n",
    "        y_standardized = (y - y.mean()) / y.std()\n",
    "        predictions, losses = buildAndTrainTheModel(x, y_standardized)\n",
    "        yHat = predictions.detach()\n",
    "\n",
    "        # store the final loss and performance\n",
    "        results[slope_idx, N, 0] = losses[-1]\n",
    "        results[slope_idx, N, 1] = np.corrcoef(y.T, yHat.T)[0, 1]\n",
    "\n",
    "\n",
    "# correlation can be 0 if the model didn't do well. Set nan's->0\n",
    "results[np.isnan(results)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results!\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "# Left subplot - Loss\n",
    "ax[0].plot(\n",
    "    slopes,\n",
    "    np.mean(results[:, :, 0], axis=1),\n",
    "    \"o-\",\n",
    "    color=\"#a6da95\",\n",
    "    markerfacecolor=\"#a6da95\",\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=1.5,\n",
    "    markersize=10,\n",
    "    linewidth=3,\n",
    ")\n",
    "ax[0].set_xlabel(\"Slope\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[0].set_title(\"Loss\", fontsize=16, color=\"#ed8796\", fontweight=\"bold\", pad=15)\n",
    "ax[0].grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax[0].set_facecolor(\"#1e1e2e\")\n",
    "ax[0].tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Right subplot - Model performance\n",
    "ax[1].plot(\n",
    "    slopes,\n",
    "    np.mean(results[:, :, 1], axis=1),\n",
    "    \"s-\",\n",
    "    color=\"#C6A0F6\",\n",
    "    markerfacecolor=\"#C6A0F6\",\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=1.5,\n",
    "    markersize=10,\n",
    "    linewidth=3,\n",
    ")\n",
    "ax[1].set_xlabel(\"Slope\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[1].set_ylabel(\n",
    "    \"Real-predicted correlation\", fontsize=14, color=\"#cad3f5\", fontweight=\"bold\"\n",
    ")\n",
    "ax[1].set_title(\n",
    "    \"Model performance\", fontsize=16, color=\"#ed8796\", fontweight=\"bold\", pad=15\n",
    ")\n",
    "ax[1].grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax[1].set_facecolor(\"#1e1e2e\")\n",
    "ax[1].tick_params(colors=\"#cad3f5\", labelsize=12)\n",
    "\n",
    "# Style spines for both subplots\n",
    "for axis in ax:\n",
    "    for spine in axis.spines.values():\n",
    "        spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's move on to classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and process the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset (comes with seaborn)\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plots to show the data with Catppuccin Macchiato styling\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# Create the pairplot with custom styling\n",
    "g = sns.pairplot(\n",
    "    iris,\n",
    "    hue=\"species\",\n",
    "    palette=[\"#C6A0F6\", \"#a6da95\", \"#f5a97f\"],  # Catppuccin colors for species\n",
    "    plot_kws={\n",
    "        \"alpha\": 0.8,\n",
    "        \"s\": 50,  # marker size\n",
    "        \"edgecolors\": \"#cad3f5\",\n",
    "        \"linewidth\": 0.5,\n",
    "    },\n",
    "    diag_kws={\"alpha\": 0.7, \"edgecolor\": \"#cad3f5\"},\n",
    ")\n",
    "\n",
    "# Style the figure\n",
    "g.figure.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "# Style each subplot\n",
    "for i in range(len(g.axes)):\n",
    "    for j in range(len(g.axes[i])):\n",
    "        ax = g.axes[i, j]\n",
    "        ax.set_facecolor(\"#1e1e2e\")\n",
    "        ax.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "        ax.tick_params(colors=\"#cad3f5\", labelsize=10)\n",
    "\n",
    "        # Style spines\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(\"#cad3f5\")\n",
    "\n",
    "        # Style labels\n",
    "        if ax.get_xlabel():\n",
    "            ax.set_xlabel(ax.get_xlabel(), color=\"#cad3f5\", fontweight=\"bold\")\n",
    "        if ax.get_ylabel():\n",
    "            ax.set_ylabel(ax.get_ylabel(), color=\"#cad3f5\", fontweight=\"bold\")\n",
    "\n",
    "# Style the legend and move it outside\n",
    "legend = g._legend\n",
    "if legend:\n",
    "    legend.set_frame_on(True)\n",
    "    legend.get_frame().set_facecolor(\"#181825\")\n",
    "    legend.get_frame().set_edgecolor(\"#cad3f5\")\n",
    "    legend.get_frame().set_alpha(0.9)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(\"#cad3f5\")\n",
    "        text.set_fontweight(\"bold\")\n",
    "\n",
    "    # Move legend outside the plot area\n",
    "    legend.set_bbox_to_anchor((1.05, 1))\n",
    "\n",
    "# Add a main title\n",
    "g.figure.suptitle(\n",
    "    \"Iris Dataset: Pairwise Feature Relationships\",\n",
    "    fontsize=16,\n",
    "    color=\"#ed8796\",\n",
    "    fontweight=\"bold\",\n",
    "    y=0.98,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize the data\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "labels[iris.species == \"versicolor\"] = 1\n",
    "labels[iris.species == \"virginica\"] = 2\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "ANNiris = nn.Sequential(\n",
    "    nn.Linear(4, 64),  # input layer\n",
    "    nn.ReLU(),  # activation\n",
    "    nn.Linear(64, 64),  # hidden layer\n",
    "    nn.ReLU(),  # activation\n",
    "    nn.Linear(64, 3),  # output layer\n",
    ")\n",
    "\n",
    "# loss function\n",
    "lossfun = (\n",
    "    nn.CrossEntropyLoss()\n",
    ")  # This method performs a Logsoftmax on the output and then computes the cross-entropy loss.\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(ANNiris.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss is a metric used to measure the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "It is the most common loss function for classification problems.\n",
    "\n",
    "***\n",
    "\n",
    "### The Core Idea: Measuring \"Surprise\" 🤔\n",
    "\n",
    "At its heart, cross-entropy measures the \"surprise\" the model feels when it sees the actual outcome. The goal is to train a model that is not surprised by the truth.\n",
    "\n",
    "* **Low Loss (Low Surprise)**: If the model predicts a high probability for the correct class (e.g., predicts 95% \"cat\" and the image is a cat), the surprise is low, and therefore the loss is low.\n",
    "* **High Loss (High Surprise)**: If the model predicts a low probability for the correct class (e.g., predicts 5% \"cat\" and the image is a cat), the surprise is high. The loss is also high, heavily penalizing the model for being confidently wrong.\n",
    "\n",
    "This penalty for confident mistakes is what makes cross-entropy so effective. The logarithm in its formula means that as the predicted probability for the correct answer gets closer to 0, the loss approaches infinity.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### How It Works in Practice\n",
    "\n",
    "Cross-entropy comes in two main flavors depending on the type of classification task.\n",
    "\n",
    "#### 1. Binary Cross-Entropy\n",
    "\n",
    "* **When to use it**: For **two-class** classification problems (e.g., spam or not spam, cat or dog).\n",
    "* **How it works**: The model outputs a single probability value for the positive class (usually via a `Sigmoid` activation function). The loss function then considers two scenarios:\n",
    "    * If the true label is 1, it calculates `-log(predicted_probability)`.\n",
    "    * If the true label is 0, it calculates `-log(1 - predicted_probability)`.\n",
    "* **In PyTorch**: `nn.BCELoss()` or `nn.BCEWithLogitsLoss()` (which is more numerically stable and recommended).\n",
    "\n",
    "#### 2. Categorical Cross-Entropy\n",
    "\n",
    "* **When to use it**: For **multi-class** classification problems where each sample belongs to only one class (e.g., classifying images of digits 0-9).\n",
    "* **How it works**: The model outputs a probability distribution across all classes (usually via a `Softmax` activation function). The loss calculation then simplifies to just looking at the predicted probability for the **one true class**.\n",
    "    * The formula is simply: `-log(probability_of_the_correct_class)`.\n",
    "* **In PyTorch**: `nn.CrossEntropyLoss()` (which conveniently includes the Softmax activation within its calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numepochs = 1000\n",
    "\n",
    "# initialize losses\n",
    "losses = torch.zeros(numepochs)\n",
    "accuracies = []\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    predictions = ANNiris(data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(predictions, labels)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute accuracy (For every epoch)\n",
    "    matches = torch.argmax(predictions, axis=1) == labels  # booleans (false/true)\n",
    "    # argmax returns the index of the maximum value in each row of yHat\n",
    "    # so if the predicted label is the same as the true label, the match is true\n",
    "    matchesNumeric = matches.float()  # convert to numbers (0/1)\n",
    "    accuracyPct = 100 * torch.mean(matchesNumeric)  # average and x100\n",
    "    accuracies.append(accuracyPct)  # add to list of accuracies\n",
    "\n",
    "\n",
    "# final forward pass\n",
    "predictions = ANNiris(data)\n",
    "\n",
    "predlabels = torch.argmax(predictions, axis=1)\n",
    "totalacc = 100 * torch.mean((predlabels == labels).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report accuracy\n",
    "print(\"Final accuracy: %g%%\" % totalacc)\n",
    "\n",
    "# Apply Catppuccin Macchiato styling\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 4), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "# Loss plot\n",
    "ax[0].plot(\n",
    "    losses.detach(),\n",
    "    color=\"#a6da95\",\n",
    "    linewidth=2,\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=0.5,\n",
    ")\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[0].set_xlabel(\"Epoch\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[0].set_title(\n",
    "    \"Training Loss\", fontsize=14, color=\"#ed8796\", fontweight=\"bold\", pad=15\n",
    ")\n",
    "ax[0].grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax[0].set_facecolor(\"#1e1e2e\")\n",
    "ax[0].tick_params(colors=\"#cad3f5\", labelsize=10)\n",
    "\n",
    "# Accuracy plot\n",
    "ax[1].plot(\n",
    "    accuracies,\n",
    "    color=\"#C6A0F6\",\n",
    "    linewidth=2,\n",
    "    marker=\"s\",\n",
    "    markersize=3,\n",
    "    markeredgecolor=\"#cad3f5\",\n",
    "    markeredgewidth=0.5,\n",
    ")\n",
    "ax[1].set_ylabel(\"Accuracy (%)\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[1].set_xlabel(\"Epoch\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "ax[1].set_title(\n",
    "    \"Training Accuracy\", fontsize=14, color=\"#ed8796\", fontweight=\"bold\", pad=15\n",
    ")\n",
    "ax[1].grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "ax[1].set_facecolor(\"#1e1e2e\")\n",
    "ax[1].tick_params(colors=\"#cad3f5\", labelsize=10)\n",
    "\n",
    "# Style spines for both subplots\n",
    "for axis in ax:\n",
    "    for spine in axis.spines.values():\n",
    "        spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# run training again to see whether this performance is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that all model predictions sum to 1, but only when converted to softmax\n",
    "sm = nn.Softmax(1)\n",
    "torch.sum(sm(predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the raw model outputs\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "fig = plt.figure(figsize=(10, 4), facecolor=\"#1e1e2e\")\n",
    "fig.patch.set_facecolor(\"#1e1e2e\")\n",
    "\n",
    "# Plot with custom colors for each species\n",
    "species_colors = [\"#C6A0F6\", \"#a6da95\", \"#f5a97f\"]  # setosa, versicolor, virginica\n",
    "softmax_outputs = sm(predictions.detach())\n",
    "\n",
    "for i, (species, color) in enumerate(\n",
    "    zip([\"setosa\", \"versicolor\", \"virginica\"], species_colors)\n",
    "):\n",
    "    plt.plot(\n",
    "        softmax_outputs[:, i],\n",
    "        \"s-\",\n",
    "        color=color,\n",
    "        markerfacecolor=color,\n",
    "        markeredgecolor=\"#cad3f5\",\n",
    "        markeredgewidth=0.5,\n",
    "        linewidth=2,\n",
    "        markersize=6,\n",
    "        label=species,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Stimulus number\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Probability\", fontsize=12, color=\"#cad3f5\", fontweight=\"bold\")\n",
    "plt.title(\n",
    "    \"Model Output Probabilities\",\n",
    "    fontsize=14,\n",
    "    color=\"#ed8796\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=15,\n",
    ")\n",
    "plt.grid(True, alpha=0.3, color=\"#cad3f5\", linestyle=\"--\")\n",
    "plt.gca().set_facecolor(\"#1e1e2e\")\n",
    "plt.gca().tick_params(colors=\"#cad3f5\", labelsize=10)\n",
    "\n",
    "# Style legend\n",
    "legend = plt.legend(\n",
    "    fontsize=11, framealpha=0.9, facecolor=\"#181825\", edgecolor=\"#cad3f5\"\n",
    ")\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(\"#cad3f5\")\n",
    "    text.set_fontweight(\"bold\")\n",
    "\n",
    "# Style spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color(\"#cad3f5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# try it again without the softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) When the loss does not reach an asymptote, it's a good idea to train the model for more epochs. Increase the number of\n",
    "#    epochs until the plot of the losses seems to hit a \"floor\" (that's a statistical term for being as small as possible).\n",
    "\n",
    "# 2) We used a model with 64 hidden units. Modify the code to have 16 hidden units. How does this model perform? If there\n",
    "#    is a decrease in accuracy, is that decrease distributed across all three iris types, or does the model learn some\n",
    "#    iris types and not others?\n",
    "\n",
    "# 3) Write code to compute three accuracy scores, one for each iris type. In real DL projects, category-specific accuracies\n",
    "#    are often more informative than the aggregated accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615884593383
    }
   ]
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
