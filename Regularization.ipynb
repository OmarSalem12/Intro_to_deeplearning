{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d18bbc",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "- Penalizes memorization of training examples\n",
    "- Penalizes the complexity of the solution and forces solutions to be smooth\n",
    "- Helps the model generalize to new data (effect on training accuracy ?)\n",
    "- Changes the representation of learning (feature space) \n",
    "- Works better with larger models with multiple hidden layers, can be counterproductive with smaller models \n",
    "- Works better with sufficient data\n",
    "\n",
    "### Three kinds of regularization \n",
    "\n",
    "- Node regularization \n",
    "- Loss regularization \n",
    "- Data regularization\n",
    "\n",
    "#### Node regularization: Dropouts \n",
    "\n",
    "- In essence we force the activation of some nodes at random to zero \n",
    "- We use a fixed probability of dropout per node (not collectivelly per layer)\n",
    "- No dropout happens during testing\n",
    "- Makes the model less reliant on individual nodes i.e. forces a distributed represenatation across nodes\n",
    "- Prevents a single node from learning too much \n",
    "- Works better with deeper networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff145614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf80437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "# labels[iris.species=='setosa'] = 0 # don't need!\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b589e5",
   "metadata": {},
   "source": [
    "# Separate the data into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.2)\n",
    "\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_data = torch.utils.data.TensorDataset(train_data,train_labels)\n",
    "test_data  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
    "\n",
    "\n",
    "# finally, translate into dataloader objects\n",
    "batchsize    = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ec1d9",
   "metadata": {},
   "source": [
    "# Create the model and a training regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f062ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new way to create the model (OOP style)\n",
    "class OurCustomModelClass(nn.Module):\n",
    "  def __init__(self,dropoutRate):\n",
    "    super().__init__()\n",
    "\n",
    "    # layers\n",
    "    self.input  = nn.Linear( 4,12)\n",
    "    self.hidden = nn.Linear(12,12)\n",
    "    self.output = nn.Linear(12, 3)\n",
    "\n",
    "    # define a dropout rate parameter\n",
    "    self.dr = dropoutRate\n",
    "\n",
    "  # forward pass\n",
    "  def forward(self,x):\n",
    "\n",
    "    # input\n",
    "    x = F.relu( self.input(x) ) #This is different from what you've seen before, we are using F.relu instead of ReLU()\n",
    "    x = F.dropout(x,p=self.dr,training=self.training) # switch dropout off during .eval()\n",
    "    # \n",
    "\n",
    "    # hidden\n",
    "    x = F.relu( self.hidden(x) )\n",
    "    x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "    # output\n",
    "    x = self.output(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# The equivalent code using the nn.Sequential class\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(4, 12),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=dropoutRate),\n",
    "#     nn.Linear(12, 12),\n",
    "#     nn.ReLU(), \n",
    "#     nn.Dropout(p=dropoutRate),\n",
    "#     nn.Linear(12, 3)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f367702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createANewModel(dropoutrate):\n",
    "\n",
    "  # grab an instance of the model class\n",
    "  ANNiris = OurCustomModelClass(dropoutrate)\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.005)\n",
    "\n",
    "  return ANNiris,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c97ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# global parameter\n",
    "numepochs = 500\n",
    "\n",
    "def trainTheModel():\n",
    "\n",
    "  # initialize accuracies as empties (not storing losses here)\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # switch learning on\n",
    "    ANNiris.train()\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = ANNiris(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # compute training accuracy just for this batch\n",
    "      batchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # test accuracy\n",
    "    ANNiris.eval() # Very important, otherwise we evaluate on incomplete model\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    predlabels = torch.argmax( ANNiris(X),axis=1 )\n",
    "    testAcc.append( 100*torch.mean((predlabels == y).float()).item() )\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc820d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "dropoutrate = .0\n",
    "ANNiris,lossfun,optimizer = createANewModel(dropoutrate)\n",
    "\n",
    "# train the model\n",
    "trainAcc,testAcc = trainTheModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# Colors\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "train_color = '#8aadf4'  # Blue\n",
    "test_color = '#f5a97f'   # Peach\n",
    "\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(bg_color)\n",
    "\n",
    "plt.plot(trainAcc, 'o-', color=train_color, linewidth=2, markersize=6, label='Train')\n",
    "plt.plot(testAcc, 'o-', color=test_color, linewidth=2, markersize=6, label='Test')\n",
    "\n",
    "plt.xlabel('Epochs', color=text_color, fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', color=text_color, fontsize=12)\n",
    "plt.title('Dropout rate = %g'%dropoutrate, color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Style the axes\n",
    "ax.tick_params(colors=text_color)\n",
    "ax.spines['bottom'].set_color(text_color)\n",
    "ax.spines['top'].set_color(text_color)\n",
    "ax.spines['right'].set_color(text_color)\n",
    "ax.spines['left'].set_color(text_color)\n",
    "\n",
    "# Style the legend\n",
    "legend = plt.legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46456a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "\n",
    "dropoutRates = np.arange(10)/10\n",
    "results = np.zeros((len(dropoutRates),2))\n",
    "\n",
    "for di in range(len(dropoutRates)):\n",
    "\n",
    "  # create and train the model\n",
    "  ANNiris,lossfun,optimizer = createANewModel(dropoutRates[di])\n",
    "  trainAcc,testAcc = trainTheModel()\n",
    "\n",
    "  # store accuracies\n",
    "  results[di,0] = np.mean(trainAcc[-50:])\n",
    "  results[di,1] = np.mean(testAcc[-50:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the experiment results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "train_color = '#8aadf4'  # Blue\n",
    "test_color = '#f5a97f'   # Peach\n",
    "grid_color = '#5b6078'   # Surface2\n",
    "\n",
    "# Set figure and subplot backgrounds\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "for axis in ax:\n",
    "    axis.set_facecolor(bg_color)\n",
    "\n",
    "# Left subplot: Dropout rates vs accuracy\n",
    "ax[0].plot(dropoutRates, results[:,0], 'o-', color=train_color, linewidth=2, markersize=6, label='Train')\n",
    "ax[0].plot(dropoutRates, results[:,1], 'o-', color=test_color, linewidth=2, markersize=6, label='Test')\n",
    "ax[0].set_xlabel('Dropout proportion', color=text_color, fontsize=12)\n",
    "ax[0].set_ylabel('Average accuracy', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the left subplot\n",
    "ax[0].tick_params(colors=text_color)\n",
    "for spine in ax[0].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "legend1 = ax[0].legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend1.get_frame().set_alpha(0.8)\n",
    "for text in legend1.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "# Right subplot: Train-test difference\n",
    "ax[1].plot(dropoutRates, -np.diff(results,axis=1), 'o-', color='#a6da95', linewidth=2, markersize=6)  # Green\n",
    "ax[1].plot([0,.9], [0,0], '--', color=grid_color, linewidth=1.5, alpha=0.8)\n",
    "ax[1].set_xlabel('Dropout proportion', color=text_color, fontsize=12)\n",
    "ax[1].set_ylabel('Train-test difference (acc%)', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the right subplot\n",
    "ax[1].tick_params(colors=text_color)\n",
    "for spine in ax[1].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae543e6f",
   "metadata": {},
   "source": [
    "What happens if you increase the complexity of this model, for example by adding several additional (and wider) hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ab1a5",
   "metadata": {},
   "source": [
    "### Loss regularization: L1/L2 regularization\n",
    "\n",
    "- The general idea to add a penalty term to the cost function \n",
    "\n",
    "# $$\\text{Cost}_{\\text{regularized}} = \\text{Cost}_{\\text{original}}(W,b) + \\lambda \\cdot \\text{RegularizationTerm}$$\n",
    "\n",
    "- The regularization strength λ (lambda) controls the trade-off between fitting the training data well and keeping the weights small. Higher λ values lead to stronger regularization and simpler models.\n",
    "\n",
    "There are two common types of penalties\n",
    "\n",
    "#### L2 regulaization - weight decay - ridge regulization \n",
    "\n",
    "# $$\\text{Cost}_{L2} = \\text{Cost}_{\\text{original}}(W,b) + \\lambda \\sum_{i=1}^{n} w_i^2$$\n",
    "\n",
    "- L2 regularization adds a penalty term that is proportional to the sum of the squared weights. \n",
    "- This encourages the model to keep weights small (why ? & who cares ?)\n",
    "- The L2 penalty term grows quadratically with the weight values, so it tends to shrink all weights uniformly toward zero without making them exactly zero.\n",
    "\n",
    "#### L1 regularization - lasso regularization\n",
    "\n",
    "# $$\\text{Cost}_{L1} = \\text{Cost}_{\\text{original}}(W,b) + \\lambda \\sum_{i=1}^{n} |w_i|$$\n",
    " \n",
    "- L1 regularization adds a penalty term that is proportional to the sum of the absolute values of weights.\n",
    "- This encourages the model to keep weights small and can lead to sparse models (some weights become exactly zero).\n",
    "- The L1 penalty term grows linearly with the weight values, so it tends to drive some weights to exactly zero, effectively performing feature selection.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createANewModel(dropoutrate, L2lambda):\n",
    "\n",
    "  # grab an instance of the model class\n",
    "  ANNiris = OurCustomModelClass(dropoutrate)\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.005, weight_decay=L2lambda)\n",
    "\n",
    "  return ANNiris,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b06619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# global parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel():\n",
    "\n",
    "  # initialize accuracies as empties\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "  losses   = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # need to toggle train mode here??\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = ANNiris(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # compute training accuracy just for this batch\n",
    "      batchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "      batchLoss.append( loss.item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "    losses.append( np.mean(batchLoss) )\n",
    "\n",
    "    # test accuracy\n",
    "    ANNiris.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    predlabels = torch.argmax( ANNiris(X),axis=1 )\n",
    "    testAcc.append( 100*torch.mean((predlabels == y).float()).item() )\n",
    "\n",
    "    # no worries, reset here ;)\n",
    "    ANNiris.train()\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "dropoutrate = .0\n",
    "L2lambda = .01\n",
    "ANNiris,lossfun,optimizer = createANewModel(dropoutrate, L2lambda)\n",
    "\n",
    "# train the model\n",
    "trainAcc,testAcc,losses = trainTheModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "loss_color = '#f4dbd6'    # Rosewater\n",
    "train_color = '#8aadf4'   # Blue\n",
    "test_color = '#f5a97f'    # Peach\n",
    "\n",
    "# Set figure and subplot backgrounds\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "for axis in ax:\n",
    "    axis.set_facecolor(bg_color)\n",
    "\n",
    "# Left subplot: Loss\n",
    "ax[0].plot(losses, '^-', color=loss_color, linewidth=2, markersize=6)\n",
    "ax[0].set_ylabel('Loss', color=text_color, fontsize=12)\n",
    "ax[0].set_xlabel('Epochs', color=text_color, fontsize=12)\n",
    "ax[0].set_title('Losses with L2 $\\lambda$=' + str(L2lambda), color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Style the left subplot\n",
    "ax[0].tick_params(colors=text_color)\n",
    "for spine in ax[0].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "\n",
    "# Right subplot: Accuracy\n",
    "ax[1].plot(trainAcc, 'o-', color=train_color, linewidth=2, markersize=6, label='Train')\n",
    "ax[1].plot(testAcc, 'o-', color=test_color, linewidth=2, markersize=6, label='Test')\n",
    "ax[1].set_title('Accuracy with L2 $\\lambda$=' + str(L2lambda), color=text_color, fontsize=14, fontweight='bold')\n",
    "ax[1].set_xlabel('Epochs', color=text_color, fontsize=12)\n",
    "ax[1].set_ylabel('Accuracy (%)', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the right subplot\n",
    "ax[1].tick_params(colors=text_color)\n",
    "for spine in ax[1].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "legend = ax[1].legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter\n",
    "def smooth(x,k):\n",
    "  return np.convolve(x,np.ones(k)/k,mode='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9150beb",
   "metadata": {},
   "source": [
    "## Parameteric experiment to test a range of L2 regularization terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of L2 regularization amounts\n",
    "l2lambdas = np.linspace(0,.1,10)\n",
    "\n",
    "# initialize output results matrices\n",
    "accuracyResultsTrain = np.zeros((numepochs,len(l2lambdas)))\n",
    "accuracyResultsTest  = np.zeros((numepochs,len(l2lambdas)))\n",
    "\n",
    "\n",
    "# loop over batch sizes\n",
    "for li in range(len(l2lambdas)):\n",
    "\n",
    "  # create and train a model\n",
    "  ANNiris,lossfun,optimizer = createANewModel(dropoutrate=0,  L2lambda=l2lambdas[li])\n",
    "  trainAcc,testAcc,losses = trainTheModel()\n",
    "\n",
    "  # store data\n",
    "  accuracyResultsTrain[:,li] = smooth(trainAcc,10)\n",
    "  accuracyResultsTest[:,li]  = smooth(testAcc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some results\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "grid_color = '#5b6078'  # Surface2\n",
    "# Generate a range of colors from the Catppuccin palette\n",
    "colors = ['#8aadf4', '#f5a97f', '#a6da95', '#eed49f', '#f4dbd6', '#c6a0f6', '#ed8796', '#91d7e3', '#7dc4e4', '#8bd5ca']\n",
    "\n",
    "# Set figure and subplot backgrounds\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "for axis in ax:\n",
    "    axis.set_facecolor(bg_color)\n",
    "\n",
    "# Plot with different colors for each L2 lambda value\n",
    "for i in range(accuracyResultsTrain.shape[1]):\n",
    "    ax[0].plot(accuracyResultsTrain[:,i], color=colors[i % len(colors)], linewidth=2)\n",
    "    ax[1].plot(accuracyResultsTest[:,i], color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "ax[0].set_title('Train accuracy', color=text_color, fontsize=14, fontweight='bold')\n",
    "ax[1].set_title('Test accuracy', color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "# make the legend easier to read\n",
    "leglabels = [np.round(i,2) for i in l2lambdas]\n",
    "\n",
    "# common features\n",
    "for i in range(2):\n",
    "    # Style the legend\n",
    "    legend = ax[i].legend(leglabels, facecolor=bg_color, edgecolor=text_color)\n",
    "    legend.get_frame().set_alpha(0.8)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(text_color)\n",
    "    \n",
    "    # Style labels and axes\n",
    "    ax[i].set_xlabel('Epoch', color=text_color, fontsize=12)\n",
    "    ax[i].set_ylabel('Accuracy (%)', color=text_color, fontsize=12)\n",
    "    ax[i].set_ylim([50,101])\n",
    "    \n",
    "    # Style grid and ticks\n",
    "    ax[i].grid(True, color=grid_color, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    ax[i].tick_params(colors=text_color)\n",
    "    \n",
    "    # Style spines\n",
    "    for spine in ax[i].spines.values():\n",
    "        spine.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38127a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show average accuracy by L2 rate\n",
    "\n",
    "# average only some epochs\n",
    "epoch_range = [500,950]\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "train_color = '#8aadf4'  # Blue\n",
    "test_color = '#f5a97f'   # Peach\n",
    "\n",
    "# Set figure background\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(bg_color)\n",
    "\n",
    "plt.plot(l2lambdas,\n",
    "         np.mean(accuracyResultsTrain[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'o-', color=train_color, linewidth=2, markersize=8, label='TRAIN')\n",
    "\n",
    "plt.plot(l2lambdas,\n",
    "         np.mean(accuracyResultsTest[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'o-', color=test_color, linewidth=2, markersize=8, label='TEST')\n",
    "\n",
    "plt.xlabel('L2 regularization amount', color=text_color, fontsize=12)\n",
    "plt.ylabel('Accuracy', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the axes\n",
    "ax.tick_params(colors=text_color)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(text_color)\n",
    "\n",
    "# Style the legend\n",
    "legend = plt.legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f17115",
   "metadata": {},
   "source": [
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b621411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that creates the ANN model\n",
    "\n",
    "def createANewModel():\n",
    "\n",
    "  # model architecture\n",
    "  ANNiris = nn.Sequential(\n",
    "      nn.Linear(4,64),   # input layer\n",
    "      nn.ReLU(),         # activation unit\n",
    "      nn.Linear(64,64),  # hidden layer\n",
    "      nn.ReLU(),         # activation unit\n",
    "      nn.Linear(64,3),   # output units\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.005)\n",
    "\n",
    "  return ANNiris,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f90f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# global parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel(L1lambda):\n",
    "\n",
    "  # initialize accuracies as empties\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "  losses   = []\n",
    "\n",
    "  # count the total number of weights in the model\n",
    "  nweights = 0\n",
    "  for pname,weight in ANNiris.named_parameters():\n",
    "    if 'bias' not in pname:\n",
    "      nweights = nweights + weight.numel()\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = ANNiris(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "\n",
    "\n",
    "      ### add L1 term\n",
    "      L1_term = torch.tensor(0.,requires_grad=True)\n",
    "\n",
    "      # sum up all abs(weights)\n",
    "      for pname,weight in ANNiris.named_parameters():\n",
    "        if 'bias' not in pname:\n",
    "           L1_term = L1_term + torch.sum(torch.abs(weight))\n",
    "\n",
    "      # add to loss term\n",
    "      loss = loss + L1lambda*L1_term/nweights\n",
    "\n",
    "\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # compute training accuracy just for this batch\n",
    "      batchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "      batchLoss.append( loss.item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "    losses.append( np.mean(batchLoss) )\n",
    "\n",
    "    # test accuracy\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    predlabels = torch.argmax( ANNiris(X),axis=1 )\n",
    "    testAcc.append( 100*torch.mean((predlabels == y).float()).item() )\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceca123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "ANNiris,lossfun,optimizer = createANewModel()\n",
    "\n",
    "# train the model\n",
    "L1lambda = .001\n",
    "trainAcc,testAcc,losses = trainTheModel(L1lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "loss_color = '#f4dbd6'    # Rosewater\n",
    "train_color = '#8aadf4'   # Blue\n",
    "test_color = '#f5a97f'    # Peach\n",
    "\n",
    "# Set figure and subplot backgrounds\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "for axis in ax:\n",
    "    axis.set_facecolor(bg_color)\n",
    "\n",
    "# Left subplot: Loss\n",
    "ax[0].plot(losses, '^-', color=loss_color, linewidth=2, markersize=6)\n",
    "ax[0].set_ylabel('Loss', color=text_color, fontsize=12)\n",
    "ax[0].set_xlabel('Epochs', color=text_color, fontsize=12)\n",
    "ax[0].set_title('Losses with L1 $\\lambda$=' + str(L1lambda), color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Style the left subplot\n",
    "ax[0].tick_params(colors=text_color)\n",
    "for spine in ax[0].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "\n",
    "# Right subplot: Accuracy\n",
    "ax[1].plot(trainAcc, 'o-', color=train_color, linewidth=2, markersize=6, label='Train')\n",
    "ax[1].plot(testAcc, 'o-', color=test_color, linewidth=2, markersize=6, label='Test')\n",
    "ax[1].set_title('Accuracy with L1 $\\lambda$=' + str(L1lambda), color=text_color, fontsize=14, fontweight='bold')\n",
    "ax[1].set_xlabel('Epochs', color=text_color, fontsize=12)\n",
    "ax[1].set_ylabel('Accuracy (%)', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the right subplot\n",
    "ax[1].tick_params(colors=text_color)\n",
    "for spine in ax[1].spines.values():\n",
    "    spine.set_color(text_color)\n",
    "legend = ax[1].legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 1D smoothing filter\n",
    "def smooth(x,k):\n",
    "  return np.convolve(x,np.ones(k)/k,mode='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed605d83",
   "metadata": {},
   "source": [
    "## Parameteric experiment to test a range of L1 regularization terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of L1 regularization amounts\n",
    "L1lambda = np.linspace(0,.005,10)\n",
    "\n",
    "# initialize output results matrices\n",
    "accuracyResultsTrain = np.zeros((numepochs,len(L1lambda)))\n",
    "accuracyResultsTest  = np.zeros((numepochs,len(L1lambda)))\n",
    "\n",
    "\n",
    "# loop over batch sizes\n",
    "for li in range(len(L1lambda)):\n",
    "\n",
    "  # create and train a model\n",
    "  ANNiris,lossfun,optimizer = createANewModel()\n",
    "  trainAcc,testAcc,losses = trainTheModel(L1lambda[li])\n",
    "\n",
    "  # store data\n",
    "  accuracyResultsTrain[:,li] = smooth(trainAcc,10)\n",
    "  accuracyResultsTest[:,li]  = smooth(testAcc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some results\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "grid_color = '#5b6078'  # Surface2\n",
    "# Generate a range of colors from the Catppuccin palette\n",
    "colors = ['#8aadf4', '#f5a97f', '#a6da95', '#eed49f', '#f4dbd6', '#c6a0f6', '#ed8796', '#91d7e3', '#7dc4e4', '#8bd5ca']\n",
    "\n",
    "# Set figure and subplot backgrounds\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "for axis in ax:\n",
    "    axis.set_facecolor(bg_color)\n",
    "\n",
    "# Plot with different colors for each L1 lambda value\n",
    "for i in range(accuracyResultsTrain.shape[1]):\n",
    "    ax[0].plot(accuracyResultsTrain[:,i], color=colors[i % len(colors)], linewidth=2)\n",
    "    ax[1].plot(accuracyResultsTest[:,i], color=colors[i % len(colors)], linewidth=2)\n",
    "\n",
    "ax[0].set_title('Train accuracy', color=text_color, fontsize=14, fontweight='bold')\n",
    "ax[1].set_title('Test accuracy', color=text_color, fontsize=14, fontweight='bold')\n",
    "\n",
    "# make the legend easier to read\n",
    "leglabels = [np.round(i,4) for i in L1lambda]\n",
    "\n",
    "# common features\n",
    "for i in range(2):\n",
    "    # Style the legend\n",
    "    legend = ax[i].legend(leglabels, facecolor=bg_color, edgecolor=text_color)\n",
    "    legend.get_frame().set_alpha(0.8)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(text_color)\n",
    "    \n",
    "    # Style labels and axes\n",
    "    ax[i].set_xlabel('Epoch', color=text_color, fontsize=12)\n",
    "    ax[i].set_ylabel('Accuracy (%)', color=text_color, fontsize=12)\n",
    "    ax[i].set_ylim([50,101])\n",
    "    \n",
    "    # Style grid and ticks\n",
    "    ax[i].grid(True, color=grid_color, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    ax[i].tick_params(colors=text_color)\n",
    "    \n",
    "    # Style spines\n",
    "    for spine in ax[i].spines.values():\n",
    "        spine.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show average accuracy by L1 rate\n",
    "\n",
    "# average only some epochs\n",
    "epoch_range = [500,950]\n",
    "\n",
    "# Catppuccin Macchiato color scheme\n",
    "bg_color = '#24273a'\n",
    "text_color = '#cad3f5'\n",
    "train_color = '#8aadf4'  # Blue\n",
    "test_color = '#f5a97f'   # Peach\n",
    "\n",
    "# Set figure background\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "fig.patch.set_facecolor(bg_color)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor(bg_color)\n",
    "\n",
    "plt.plot(L1lambda,\n",
    "         np.mean(accuracyResultsTrain[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'o-', color=train_color, linewidth=2, markersize=8, label='TRAIN')\n",
    "\n",
    "plt.plot(L1lambda,\n",
    "         np.mean(accuracyResultsTest[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'o-', color=test_color, linewidth=2, markersize=8, label='TEST')\n",
    "\n",
    "plt.xlabel('L1 regularization amount', color=text_color, fontsize=12)\n",
    "plt.ylabel('Accuracy', color=text_color, fontsize=12)\n",
    "\n",
    "# Style the axes\n",
    "ax.tick_params(colors=text_color)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(text_color)\n",
    "\n",
    "# Style the legend\n",
    "legend = plt.legend(facecolor=bg_color, edgecolor=text_color)\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "for text in legend.get_texts():\n",
    "    text.set_color(text_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62886928",
   "metadata": {},
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b90bf",
   "metadata": {},
   "source": [
    "1) Can you modify the code here to create a manual L2 regularizer.\n",
    "\n",
    "2) Can you take a crack at trying to implement elastic net regularization ? Please see the equation below\n",
    "\n",
    "3) In the equation I provided, I specified separate parameters for L1 and L2 parameters. what if we would like to control the regularization generally (the combined regularization) but keep control over the balance between L1 & L2. Can you think of a way to modify the equation to accomodate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ff03b",
   "metadata": {},
   "source": [
    "#### Elastic Net regularization\n",
    " \n",
    "# $$\\text{Cost}_{\\text{ElasticNet}} = \\text{Cost}_{\\text{original}}(W,b) + \\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2$$\n",
    " \n",
    "- Elastic Net regularization combines both L1 and L2 penalties in a single regularization term.\n",
    "- This provides a balance between the sparsity-inducing properties of L1 (feature selection) and the weight shrinkage properties of L2.\n",
    "- The relative importance of L1 vs L2 regularization is controlled by the ratio of λ₁ and λ₂ parameters.\n",
    "- Elastic Net is particularly useful when dealing with correlated features, as it tends to select groups of correlated features together. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
